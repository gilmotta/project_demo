#!/usr/bin/env python

import os, math, sys, operator
from collections import Counter

class ngram_language_model:

    ''' We use Laplace add-one smoothing '''
    # convert a freq table to a probability table with word size
    def uni_prob_table(self, freq_table, N):
        prob_table = {}
        V = len(freq_table.keys())
        for w in freq_table:
            prob_table[w] = (freq_table[w] + 1)/ (N + V)
        return prob_table

    def bi_prob_table(self, bigram_freq_table, unigram_freq_table):
        prob_table = {}
        V = len(bigram_freq_table.keys())
        for (wi, wj) in bigram_freq_table:
            prob_table[(wi, wj)] = (
                (bigram_freq_table[(wi, wj)] + 1)/(unigram_freq_table[wi] + V))
        return prob_table

    def tri_prob_table(self, trigram_freq_table, bigram_freq_table):
        prob_table = {}
        V = len(trigram_freq_table.keys())
        for (wi, wj, wk) in trigram_freq_table:
            prob_table[(wi, wj, wk)] = (
                (trigram_freq_table[(wi, wj, wk)] + 1)/(bigram_freq_table[(wj, wk)] + V)
            )
        return prob_table

    # smoothed bigram freq table
    def bigram_freq_table(self, training_acc):
        bigram_training_acc = []
        for i in range(1, len(training_acc)):
            bigram_training_acc.append((training_acc[i-1], training_acc[i]))
        return Counter(bigram_training_acc)

    # smoothed trigram freq table
    def trigram_freq_table(self, training_acc):
        trigram_training_acc = []
        for i in range(2, len(training_acc)):
            trigram_training_acc.append((training_acc[i - 2], 
                                         training_acc[i - 1],
                                         training_acc[i]))
        return Counter(trigram_training_acc)

    # return a list of words as strings
    def get_words(self, path):
        with open(path, "r") as file:
            acc = []
            for line in file:
                line = line.lower()
                words = line.split(' ')
                acc += words
        return acc

    def infrequent_word_handler(self, training_acc):
        freq_table = Counter(training_acc)
        infrequent_words = set()
        for word in freq_table.keys():
            if freq_table[word] <= 5:
                infrequent_words.add(word)
        for i in range(len(training_acc)):
            if training_acc[i] in infrequent_words:
                training_acc[i] = "unknown"

    # prepare 3-gram language model with 3 probability tables
    def train(self, training_acc):
        self.infrequent_word_handler(training_acc)
        unigram_freq_table = Counter(training_acc)
        ''' debugging code: print sorted freq_table '''
        # print(sorted(unigram_freq_table.items(), key = operator.itemgetter(1)))
        bigram_freq_table = self.bigram_freq_table(training_acc)
        # print(sorted(bigram_freq_table.items(), key = operator.itemgetter(1)))
        trigram_freq_table = self.trigram_freq_table(training_acc)
        print(sorted(trigram_freq_table.items(), key = operator.itemgetter(1)))
        # n : number of words, vocab_size : number of unique words
        vocab_size = len(unigram_freq_table.keys())
        N = len(training_acc)
        uni_prob_table = self.uni_prob_table(unigram_freq_table, N)
        bi_prob_table = self.bi_prob_table(bigram_freq_table, unigram_freq_table)
        tri_prob_table = self.tri_prob_table(trigram_freq_table, bigram_freq_table)
        language_model = (vocab_size, uni_prob_table, 
                          bi_prob_table, tri_prob_table)
        # print("# type of unigram: ", str(vocab_size))
        # print("# type of bigram : ", str(len(bi_prob_table.keys())))
        # print("# type of trigram: ", str(len(tri_prob_table.keys())))
        return language_model

    # test takes sys inputs and returns perplexity of the trianing file
    def test(self, lamd0, lamd1, lamd2, lamd3, test, mod, default = 0):
        vocab_size, uni_prob_table, bi_prob_table, tri_prob_table = (
            mod[0], mod[1], mod[2], mod[3])
        v_bi = len(bi_prob_table.keys())
        v_tri = len(tri_prob_table.keys())
        if default == 0:
            w = self.get_words(test)
        else:
            w = test
        w = [w_i if w_i in uni_prob_table.keys() else 'unknown' for w_i in w]
        w_size = len(w)
        log_product = 0
        for i in range(w_size):
            acc = float(lamd0) / vocab_size
            if i >= 2:
                """ Laplace Smoothing: in test data, if incoming data is new
                    p = lamd3 * ((1 + 0) / (C(uni(wi)) + v_tri) ) """ 
                acc += lamd3 * tri_prob_table.get((w[i-2],w[i-1],w[i]),
                       1.0 / (bi_prob_table.get((w[i-1],w[i]), 0) + v_tri))
            if i >= 1:
                acc += lamd2 * bi_prob_table.get((w[i-1], w[i]), 
                        1.0/(uni_prob_table.get(w[i], 0) + v_bi))
            if i >= 0:
                acc += lamd1 * uni_prob_table.get(w[i], 1.0/(w_size + vocab_size))
            if acc == 0:
                continue
            # log of product = sum of log
            log_product += math.log(acc, 2)
        # log(p^-1/w) = (-log(p)/w) by log rule, then unlog and compute perplexity
        perplexity = 2 ** (-log_product / float(w_size))
        print(perplexity)

    def collapse_file(self, files):
        acc = []
        for file_path in files:
            acc += self.get_words(file_path)
        return acc

    def split_file(self, single_file):
        with open(single_file, "r") as f:
            train = []
            test = []
            count = 0
            for line in f:
                line = line.lower()
                words = line.split(" ")
                if count < 25:
                    train += words
                else:
                    test += words
                count += 1
        return (train, test)

    def main(self):
        if len(sys.argv) == 7:
            language_model = self.train(self.get_words(sys.argv[5]))
            self.test(float(sys.argv[1]), float(sys.argv[2]), float(sys.argv[3]), 
                float(sys.argv[4]), sys.argv[5], language_model)
        elif len(sys.argv) == 10:
            train_words = self.collapse_file([sys.argv[6], sys.argv[7], sys.argv[8], sys.argv[9]])
            language_model = self.train(train_words)
            self.test(float(sys.argv[1]), float(sys.argv[2]), float(sys.argv[3]), 
                float(sys.argv[4]), sys.argv[5], language_model)
        elif len(sys.argv) == 6:
            (train_words, test_words) = self.split_file(sys.argv[5])
            language_model = self.train(train_words)
            self.test(float(sys.argv[1]), float(sys.argv[2]), float(sys.argv[3]), 
                float(sys.argv[4]), test_words, language_model, 1)
            # now, we flip the order of test and train
            language_model = self.train(test_words)
            self.test(float(sys.argv[1]), float(sys.argv[2]), float(sys.argv[3]), 
                float(sys.argv[4]), train_words, language_model, 1)
        else:
            print("[Zihan Guo]: Warning: Invalid sys.argv size")
            print("[Zihan Guo]: Enter one of the followings")
            print("             ./languagemodeler 0.25 0.25 0.25 0.25 news.txt test.txt train.txt")
            print("             ./languagemodeler 0.25 0.25 0.25 0.25 news.txt test.txt train1.txt train2.txt train3.txt tran4.txt")
            print("             ./languagemodeler 0.25 0.25 0.25 0.25 news.txt")

run = ngram_language_model()
run.main()


